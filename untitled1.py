# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YOstg0pHbCRMY3p4zEFJ_cB2kSs3BBNZ
"""

# =============================
# Install Required Libraries
# =============================
!pip install -q torch torchvision torchaudio transformers datasets scikit-learn librosa

import os
import torch
import torch.nn as nn
import torchaudio
import torchaudio.transforms as transforms
import librosa
import numpy as np
import zipfile
import requests
from tqdm import tqdm
from transformers import AutoModel, AutoFeatureExtractor
from sklearn.metrics import accuracy_score, classification_report
from torch.utils.data import DataLoader, TensorDataset

# =============================
# Step 1: Download & Extract Dataset
# =============================
dataset_url = "https://datashare.ed.ac.uk/bitstream/handle/10283/3336/LA.zip?sequence=3&isAllowed=y"
zip_filename = "ASVspoof2019_LA.zip"
dataset_dir = "/content/ASVspoof2019_LA"

# Download dataset
print("Downloading dataset...")
response = requests.get(dataset_url, stream=True)
with open(zip_filename, "wb") as file:
    for chunk in response.iter_content(chunk_size=1024):
        if chunk:
            file.write(chunk)

print("Dataset downloaded successfully!")

# Extract the zip file
print("Extracting dataset...")
with zipfile.ZipFile(zip_filename, "r") as zip_ref:
    zip_ref.extractall(dataset_dir)

print("Dataset extracted successfully!")

# Define paths
train_audio_path = os.path.join(dataset_dir, "LA", "train", "flac")
dev_audio_path = os.path.join(dataset_dir, "LA", "dev", "flac")

# Define protocol file paths
train_protocol_file = os.path.join(dataset_dir, "LA", "ASVspoof2019.LA.cm.train.trn.txt")
dev_protocol_file = os.path.join(dataset_dir, "LA", "ASVspoof2019.LA.cm.dev.trl.txt")

# =============================
# Step 2: Load Audio Files and Labels
# =============================
def load_dataset(audio_path, protocol_file):
    audio_files, labels = [], []
    with open(protocol_file, 'r') as f:
        for line in f:
            parts = line.strip().split()
            file_name, label = parts[1], 1 if parts[-1] == 'spoof' else 0  # Spoof = 1, Genuine = 0
            file_path = os.path.join(audio_path, file_name + ".flac")
            if os.path.exists(file_path):
                audio_files.append(file_path)
                labels.append(label)
    return audio_files, labels

# Load train & dev datasets
train_files, train_labels = load_dataset(train_audio_path, train_protocol_file)
dev_files, dev_labels = load_dataset(dev_audio_path, dev_protocol_file)

print(f"Loaded {len(train_files)} training samples and {len(dev_files)} development samples.")

# =============================
# Step 3: Load Pre-trained Wav2Vec2 Model
# =============================
model_name = "facebook/wav2vec2-base-960h"
feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)
ssl_model = AutoModel.from_pretrained(model_name)

# Freeze the SSL model's parameters (only use it as a feature extractor)
for param in ssl_model.parameters():
    param.requires_grad = False

# =============================
# Step 4: Extract Embeddings from Audio
# =============================
def extract_embedding(file_path):
    # Load audio
    waveform, sample_rate = torchaudio.load(file_path)

    # Convert sample rate if necessary
    if sample_rate != 16000:
        resample = transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resample(waveform)

    # Extract features
    input_values = feature_extractor(waveform.squeeze().numpy(), return_tensors="pt").input_values
    with torch.no_grad():
        embeddings = ssl_model(input_values).last_hidden_state.mean(dim=1)  # Mean pooling
    return embeddings.cpu().numpy().squeeze()

# Extract embeddings for all audio files
def extract_embeddings(audio_files):
    embeddings = []
    for file in tqdm(audio_files, desc="Extracting features"):
        embeddings.append(extract_embedding(file))
    return np.array(embeddings)

# Generate embeddings for train & dev sets
train_embeddings = extract_embeddings(train_files)
dev_embeddings = extract_embeddings(dev_files)

# =============================
# Step 5: Convert to Tensors
# =============================
train_embeddings_tensor = torch.tensor(train_embeddings, dtype=torch.float32)
train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)
dev_embeddings_tensor = torch.tensor(dev_embeddings, dtype=torch.float32)
dev_labels_tensor = torch.tensor(dev_labels, dtype=torch.long)

# =============================
# Step 6: Define a Simple Classifier
# =============================
class SimpleClassifier(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(SimpleClassifier, self).__init__()
        self.fc = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        return self.fc(x)

# Define model
embedding_dim = ssl_model.config.hidden_size  # Wav2Vec2 hidden size
classifier = SimpleClassifier(embedding_dim, 2)  # Binary classification (Genuine vs. Spoofed)

# =============================
# Step 7: Train the Model
# =============================
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)
epochs = 10
batch_size = 32

# Create DataLoader
train_dataset = TensorDataset(train_embeddings_tensor, train_labels_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Training loop
for epoch in range(epochs):
    classifier.train()
    total_loss = 0
    for batch_embeddings, batch_labels in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
        optimizer.zero_grad()
        outputs = classifier(batch_embeddings)
        loss = criterion(outputs, batch_labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}")

# =============================
# Step 8: Evaluate the Model
# =============================
classifier.eval()
with torch.no_grad():
    dev_outputs = classifier(dev_embeddings_tensor)
    _, predicted = torch.max(dev_outputs, 1)
    accuracy = accuracy_score(dev_labels_tensor.cpu().numpy(), predicted.cpu().numpy())
    print(f"\nDevelopment Set Accuracy: {accuracy:.4f}")
    print(classification_report(dev_labels_tensor.cpu().numpy(), predicted.cpu().numpy()))